func=cv_score,                 # Función objetivo
dimensions=search_space,       # Espacio de búsqueda
acq_func='EI',                 # Expected Improvement
n_calls=20,                    # Número de iteraciones
n_initial_points=5,            # Puntos iniciales aleatorios
random_state=1234                # Reproducibilidad
)
# Resultados de la optimización
best_parameters = {dim.name: val for dim, val in zip(search_space, result.x)}
best_parameters['Best Negative MSE'] = result.fun
df_results = pd.DataFrame([best_parameters])
df_results
# Máximos acumulados para Random Search
# Asegúrate de que 'mean_test_score' esté disponible en tu búsqueda aleatoria
if 'mean_test_score' in rs.cv_results_:
y_rs = np.maximum.accumulate(rs.cv_results_['mean_test_score'])
else:
y_rs = None
# Máximos acumulados para la optimización bayesiana
y_bo = np.maximum.accumulate(-np.array(result.func_vals))
# Imprimir los resultados
baseline = -np.mean(cross_val_score(XGBRegressor(), X, Y, scoring='neg_mean_squared_error'))
print(f'Baseline neg. MSE = {baseline:.2f}')
if y_rs is not None:
print(f'Random search neg. MSE = {y_rs[-1]:.2f}')
print(f'Bayesian optimization skopt neg. MSE = {y_bo[-1]:.2f}')
# Graficar los resultados
plt.figure(figsize=(8, 6))
if y_rs is not None:
plt.plot(y_rs, 'ro-', label='Random search')
plt.plot(y_bo, 'bo-', label='Bayesian optimization skopt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.ylim(-5000, -3000)  # Ajusta estos límites según tu problema
plt.title('Value of the Best Sampled CV Score')
plt.legend()
plt.show()
# Máximos acumulados para Random Search
# Asegúrate de que 'mean_test_score' esté disponible en tu búsqueda aleatoria
if 'mean_test_score' in rs.cv_results_:
y_rs = np.maximum.accumulate(rs.cv_results_['mean_test_score'])
else:
y_rs = None
# Máximos acumulados para la optimización bayesiana
y_bo = np.maximum.accumulate(-np.array(result.func_vals))
baseline = -np.mean(cross_val_score(XGBRegressor(), X, Y, scoring = 'neg_mean_squared_error'))
best_results = {
"Method": ["Baseline", "Random Search", "Bayesian Optimization"],
"Neg. MSE": [
baseline,
y_rs[-1] if y_rs is not None else None,
y_bo[-1]
]
}
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results)
df_best_results
# Graficar los resultados
plt.figure(figsize=(8, 6))
if y_rs is not None:
plt.plot(y_rs, 'ro-', label='Random search')
plt.plot(y_bo, 'bo-', label='Bayesian optimization skopt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.ylim(-5000, -3000)  # Ajusta estos límites según tu problema
plt.title('Value of the Best Sampled CV Score')
plt.legend()
plt.show()
import numpy as np
import pandas as pd
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
# Define los dominios de búsqueda (bounds)
search_space = [
Real(0, 1, name='learning_rate'),               # learning_rate
Real(0, 5, name='gamma'),                       # gamma
Integer(1, 50, name='max_depth'),               # max_depth
Integer(1, 300, name='n_estimators'),           # n_estimators
Integer(1, 10, name='min_child_weight')         # min_child_weight
]
# Función objetivo para minimizar (se negará el MSE para que sea una minimización)
@use_named_args(search_space)
def cv_score(learning_rate, gamma, max_depth, n_estimators, min_child_weight):
model = XGBRegressor(
learning_rate=learning_rate,
gamma=gamma,
max_depth=max_depth,
n_estimators=n_estimators,
min_child_weight=min_child_weight
)
score = cross_val_score(model, X, Y, scoring='neg_mean_squared_error').mean()
return -score  # Negamos el puntaje para minimizar
# Ejecutar la optimización bayesiana
result = gp_minimize(
func=cv_score,                 # Función objetivo
dimensions=search_space,       # Espacio de búsqueda
acq_func='EI',                 # Expected Improvement
n_calls=20,                    # Número de iteraciones
n_initial_points=5,            # Puntos iniciales aleatorios
random_state=1234                # Reproducibilidad
)
# Resultados de la optimización
best_parameters = {dim.name: val for dim, val in zip(search_space, result.x)}
best_parameters['Best Negative MSE'] = result.fun
df_results = pd.DataFrame([best_parameters])
df_results = df_results.rename(columns={
'learning_rate': 'Learning Rate',
'gamma': 'Gamma',
'max_depth': 'Max Depth',
'n_estimators': 'N Estimators',
'min_child_weight': 'Min Child Weight',
'Best Negative MSE': 'Best Neg. MSE'
})
df_results_t = df_results.T.reset_index()
df_results_t.columns = ['Hyperparameter', 'Value']
df_results_t.index=False
import numpy as np
import pandas as pd
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
# Define los dominios de búsqueda (bounds)
search_space = [
Real(0, 1, name='learning_rate'),               # learning_rate
Real(0, 5, name='gamma'),                       # gamma
Integer(1, 50, name='max_depth'),               # max_depth
Integer(1, 300, name='n_estimators'),           # n_estimators
Integer(1, 10, name='min_child_weight')         # min_child_weight
]
# Función objetivo para minimizar (se negará el MSE para que sea una minimización)
@use_named_args(search_space)
def cv_score(learning_rate, gamma, max_depth, n_estimators, min_child_weight):
model = XGBRegressor(
learning_rate=learning_rate,
gamma=gamma,
max_depth=max_depth,
n_estimators=n_estimators,
min_child_weight=min_child_weight
)
score = cross_val_score(model, X, Y, scoring='neg_mean_squared_error').mean()
return -score  # Negamos el puntaje para minimizar
# Ejecutar la optimización bayesiana
result = gp_minimize(
func=cv_score,                 # Función objetivo
dimensions=search_space,       # Espacio de búsqueda
acq_func='EI',                 # Expected Improvement
n_calls=20,                    # Número de iteraciones
n_initial_points=5,            # Puntos iniciales aleatorios
random_state=1234                # Reproducibilidad
)
# Resultados de la optimización
best_parameters = {dim.name: val for dim, val in zip(search_space, result.x)}
best_parameters['Best Negative MSE'] = result.fun
df_results = pd.DataFrame([best_parameters])
df_results = df_results.rename(columns={
'learning_rate': 'Learning Rate',
'gamma': 'Gamma',
'max_depth': 'Max Depth',
'n_estimators': 'N Estimators',
'min_child_weight': 'Min Child Weight',
'Best Negative MSE': 'Best Neg. MSE'
})
df_results_t = df_results.T.reset_index()
df_results_t.columns = ['Hyperparameter', 'Value']
df_results_t.to_markdown(index=False)
import numpy as np
import pandas as pd
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
# Define los dominios de búsqueda (bounds)
search_space = [
Real(0, 1, name='learning_rate'),               # learning_rate
Real(0, 5, name='gamma'),                       # gamma
Integer(1, 50, name='max_depth'),               # max_depth
Integer(1, 300, name='n_estimators'),           # n_estimators
Integer(1, 10, name='min_child_weight')         # min_child_weight
]
# Función objetivo para minimizar (se negará el MSE para que sea una minimización)
@use_named_args(search_space)
def cv_score(learning_rate, gamma, max_depth, n_estimators, min_child_weight):
model = XGBRegressor(
learning_rate=learning_rate,
gamma=gamma,
max_depth=max_depth,
n_estimators=n_estimators,
min_child_weight=min_child_weight
)
score = cross_val_score(model, X, Y, scoring='neg_mean_squared_error').mean()
return -score  # Negamos el puntaje para minimizar
# Ejecutar la optimización bayesiana
result = gp_minimize(
func=cv_score,                 # Función objetivo
dimensions=search_space,       # Espacio de búsqueda
acq_func='EI',                 # Expected Improvement
n_calls=20,                    # Número de iteraciones
n_initial_points=5,            # Puntos iniciales aleatorios
random_state=1234                # Reproducibilidad
)
# Resultados de la optimización
best_parameters = {dim.name: val for dim, val in zip(search_space, result.x)}
best_parameters['Best Negative MSE'] = result.fun
df_results = pd.DataFrame([best_parameters])
df_results = df_results.rename(columns={
'learning_rate': 'Learning Rate',
'gamma': 'Gamma',
'max_depth': 'Max Depth',
'n_estimators': 'N Estimators',
'min_child_weight': 'Min Child Weight',
'Best Negative MSE': 'Best Neg. MSE'
})
df_results_t = df_results.T.reset_index()
df_results_t.columns = ['Hyperparameter', 'Value']
df_results_t
import numpy as np
import pandas as pd
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
# Define los dominios de búsqueda (bounds)
search_space = [
Real(0, 1, name='learning_rate'),               # learning_rate
Real(0, 5, name='gamma'),                       # gamma
Integer(1, 50, name='max_depth'),               # max_depth
Integer(1, 300, name='n_estimators'),           # n_estimators
Integer(1, 10, name='min_child_weight')         # min_child_weight
]
# Función objetivo para minimizar (se negará el MSE para que sea una minimización)
@use_named_args(search_space)
def cv_score(learning_rate, gamma, max_depth, n_estimators, min_child_weight):
model = XGBRegressor(
learning_rate=learning_rate,
gamma=gamma,
max_depth=max_depth,
n_estimators=n_estimators,
min_child_weight=min_child_weight
)
score = cross_val_score(model, X, Y, scoring='neg_mean_squared_error').mean()
return -score  # Negamos el puntaje para minimizar
# Ejecutar la optimización bayesiana
result = gp_minimize(
func=cv_score,                 # Función objetivo
dimensions=search_space,       # Espacio de búsqueda
acq_func='EI',                 # Expected Improvement
n_calls=20,                    # Número de iteraciones
n_initial_points=5,            # Puntos iniciales aleatorios
random_state=1234                # Reproducibilidad
)
# Resultados de la optimización
best_parameters = {dim.name: val for dim, val in zip(search_space, result.x)}
best_parameters['Best Negative MSE'] = result.fun
df_results = pd.DataFrame([best_parameters])
df_results = df_results.rename(columns={
'learning_rate': 'Learning Rate',
'gamma': 'Gamma',
'max_depth': 'Max Depth',
'n_estimators': 'N Estimators',
'min_child_weight': 'Min Child Weight',
'Best Negative MSE': 'Best Neg. MSE'
})
df_results_t = df_results.T.reset_index()
df_results_t.columns = ['Hyperparameter', 'Value']
df_results_t.reset_index(drop=True, inplace=True)
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results)
df_best_results.round(3)
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results)
df_best_results.round(3).style.format("{:.3f}")
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results).round(3)
df_best_results.style.format("{:.3f}")
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results).round(3)
df_best_results.style.format("{:.3f}")
df_best_results
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results).round(3)
df_best_results.style.format("{:,.3f}")
df_best_results
# Convertir el diccionario en un DataFrame
df_best_results = pd.DataFrame(best_results).round(3)
df_best_results.style.format("{,:.3f}")
df_best_results
df_best_results = pd.DataFrame(best_results).round(3)
df_best_results.style.format("{:,.3f}")
y_rs = np.maximum.accumulate(rs.cv_results_['mean_test_score'])
y_bo = np.maximum.accumulate(-optimizer.Y).ravel()
print(f'Baseline neg. MSE = {baseline:.2f}')
print(f'Random search neg. MSE = {y_rs[-1]:.2f}')
print(f'Bayesian optimization GPyOpt neg. MSE = {y_bo[-1]:.2f}')
plt.plot(y_rs, 'ro-', label='Random search')
plt.plot(y_bo, 'bo-', label='Bayesian optimization GPyOpt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.ylim(-5000, -3000)
plt.title('Value of the best sampled CV score');
plt.legend();
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from scipy.stats import uniform
import numpy as np
import matplotlib.pyplot as plt
# Cargar el dataset de California Housing Prices
np.random.seed(1234)
data = fetch_california_housing()
X, Y = data.data, data.target  # X: características, Y: precios de vivienda
n_features = X.shape[1]
# Instantiate an XGBRegressor with default hyperparameter settings
gbr = GradientBoostingRegressor(n_estimators = 50, random_state = 1234)
# and compute a baseline to beat with hyperparameter optimization
baseline = cross_val_score(gbr, X, Y, scoring='neg_mean_squared_error').mean()
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from GPyOpt.methods import BayesianOptimization
import numpy as np
# Cargar el dataset de California Housing Prices
np.random.seed(1234)
data = fetch_california_housing()
X, Y = data.data, data.target
n_features = X.shape[1]
# Definir el espacio de búsqueda para la optimización bayesiana
bds = [
{'name': 'learning_rate', 'type': 'continuous', 'domain': (10**-5, 10**0)},
{'name': 'max_features', 'type': 'discrete', 'domain': list(range(1, n_features + 1))},
{'name': 'max_depth', 'type': 'discrete', 'domain': list(range(1, 6))},
{'name': 'min_samples_split', 'type': 'discrete', 'domain': list(range(2, 1001))},
{'name': 'min_samples_leaf', 'type': 'discrete', 'domain': list(range(1, 1001))}
]
# Definir el objetivo para la optimización
def cv_score(parameters):
# Los parámetros se pasan como una lista de listas
parameters = parameters[0]
# Crear el modelo con los hiperparámetros
gbr = GradientBoostingRegressor(
learning_rate=parameters[0],
max_features=int(parameters[1]),
max_depth=int(parameters[2]),
min_samples_split=int(parameters[3]),
min_samples_leaf=int(parameters[4]),
n_estimators=50,  # Puedes ajustar si quieres más árboles
random_state=1234
)
# Calcular el puntaje con validación cruzada
score = -np.mean(cross_val_score(gbr, X, Y, cv=5, scoring="neg_mean_squared_error", n_jobs=-1))
return score
# Configurar el optimizador bayesiano
optimizer = BayesianOptimization(
f=cv_score,
domain=bds,
model_type='GP',
acquisition_type='EI',
acquisition_jitter=0.05,
exact_feval=True,
maximize=False  # Minimizar el MSE
)
# Ejecutar la optimización (20 iteraciones)
optimizer.run_optimization(max_iter=20)
# Mostrar los mejores resultados
print("Mejores hiperparámetros encontrados:")
print(f"Learning rate: {optimizer.X[np.argmin(optimizer.Y), 0]:.5f}")
print(f"Max features: {int(optimizer.X[np.argmin(optimizer.Y), 1])}")
print(f"Max depth: {int(optimizer.X[np.argmin(optimizer.Y), 2])}")
print(f"Min samples split: {int(optimizer.X[np.argmin(optimizer.Y), 3])}")
print(f"Min samples leaf: {int(optimizer.X[np.argmin(optimizer.Y), 4])}")
print(f"Mejor MSE (negativo): {np.min(optimizer.Y):.4f}")
# Comparar resultados de Random Search y Bayesian Optimization
y_rs = -np.maximum.accumulate(rs.cv_results_['mean_test_score'])
y_bo = -np.maximum.accumulate(-optimizer.Y).ravel()
print(f'Baseline neg. MSE = {baseline:.5f}')
print(f'Random search neg. MSE = {y_rs[-1]:.5f}')
print(f'Bayesian optimization GPyOpt neg. MSE = {y_bo[-1]:.5f}')
# Graficar
plt.plot(y_rs, 'ro-', label='Random search')
plt.plot(y_bo, 'bo-', label='Bayesian optimization GPyOpt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.title('Value of the best sampled CV score')
plt.legend()
plt.show()
# Comparar resultados de Random Search y Bayesian Optimization
y_rs = -np.maximum.accumulate(rs.cv_results_['mean_test_score'])
y_bo = -np.maximum.accumulate(-optimizer.Y).ravel()
print(f'Baseline neg. MSE = {baseline:.5f}')
print(f'Random search neg. MSE = {y_rs[-1]:.5f}')
print(f'Bayesian optimization GPyOpt neg. MSE = {y_bo[-1]:.5f}')
# Graficar
plt.plot(y_rs, 'ro-', label='Random search')
plt.plot(y_bo, 'bo-', label='Bayesian optimization GPyOpt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.title('Value of the best sampled CV score')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from scipy.stats import uniform
# Cargar el dataset de California Housing Prices
np.random.seed(1234)
data = fetch_california_housing()
X, Y = data.data, data.target  # X: características, Y: precios de vivienda
n_features = X.shape[1]
# Instantiate an XGBRegressor with default hyperparameter settings
gbr = GradientBoostingRegressor(n_estimators = 50, random_state = 1234)
# and compute a baseline to beat with hyperparameter optimization
baseline_h = cross_val_score(gbr, X, Y, scoring='neg_mean_squared_error').mean()
from scipy.stats import loguniform
np.random.seed(1234)
# Hyperparameters to tune and their ranges
param_dist_h = {'max_depth': range(1,5),
'learning_rate': loguniform(10**-5,10**0),
'max_features': range(1, n_features),
'min_samples_split': range(2 ,1000),
'min_samples_leaf':range(1 ,1000)}
rs_h = RandomizedSearchCV(gbr, param_distributions=param_dist_h,
scoring='neg_mean_squared_error', n_iter=25)
# Run random search for 25 iterations
rs_h.fit(X, Y);
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from GPyOpt.methods import BayesianOptimization
# Definir el espacio de búsqueda para la optimización bayesiana
bds_h = [
{'name': 'learning_rate', 'type': 'continuous', 'domain': (10**-5, 10**0)},
{'name': 'max_features', 'type': 'discrete', 'domain': list(range(1, n_features + 1))},
{'name': 'max_depth', 'type': 'discrete', 'domain': list(range(1, 6))},
{'name': 'min_samples_split', 'type': 'discrete', 'domain': list(range(2, 1001))},
{'name': 'min_samples_leaf', 'type': 'discrete', 'domain': list(range(1, 1001))}
]
# Definir el objetivo para la optimización
def cv_score(parameters):
# Los parámetros se pasan como una lista de listas
parameters = parameters[0]
# Crear el modelo con los hiperparámetros
gbr = GradientBoostingRegressor(
learning_rate=parameters[0],
max_features=int(parameters[1]),
max_depth=int(parameters[2]),
min_samples_split=int(parameters[3]),
min_samples_leaf=int(parameters[4]),
n_estimators=50,  # Puedes ajustar si quieres más árboles
random_state=1234
)
# Calcular el puntaje con validación cruzada
score = -np.mean(cross_val_score(gbr, X, Y, cv=5, scoring="neg_mean_squared_error", n_jobs=-1))
return score
# Configurar el optimizador bayesiano
optimizer_h = BayesianOptimization(
f=cv_score,
domain=bds_h,
model_type='GP',
acquisition_type='EI',
acquisition_jitter=0.05,
exact_feval=True,
maximize=False  # Minimizar el MSE
)
# Ejecutar la optimización (20 iteraciones)
optimizer_h.run_optimization(max_iter=20)
# Mostrar los mejores resultados
print("Mejores hiperparámetros encontrados:")
print(f"Learning rate: {optimizer_h.X[np.argmin(optimizer_h.Y), 0]:.5f}")
print(f"Max features: {int(optimizer_h.X[np.argmin(optimizer_h.Y), 1])}")
print(f"Max depth: {int(optimizer_h.X[np.argmin(optimizer_h.Y), 2])}")
print(f"Min samples split: {int(optimizer_h.X[np.argmin(optimizer_h.Y), 3])}")
print(f"Min samples leaf: {int(optimizer_h.X[np.argmin(optimizer_h.Y), 4])}")
print(f"Mejor MSE (negativo): {np.min(optimizer_h.Y):.4f}")
# Comparar resultados de Random Search y Bayesian Optimization
y_rs_h = -np.maximum.accumulate(rs_h.cv_results_['mean_test_score'])
y_bo_h = -np.maximum.accumulate(-optimizer_h.Y).ravel()
print(f'Baseline neg. MSE = {baseline_h:.5f}')
print(f'Random search neg. MSE = {y_rs_h[-1]:.5f}')
print(f'Bayesian optimization GPyOpt neg. MSE = {y_bo_h[-1]:.5f}')
# Graficar
plt.plot(y_rs_h, 'ro-', label='Random search')
plt.plot(y_bo_h, 'bo-', label='Bayesian optimization GPyOpt')
plt.xlabel('Iteration')
plt.ylabel('Neg. MSE')
plt.title('Value of the best sampled CV score')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from scipy.stats import uniform
# Cargar el dataset de California Housing Prices
np.random.seed(1234)
data_h = fetch_california_housing()
X, Y = data.data, data_h.target  # X: características, Y: precios de vivienda
n_features_h = X.shape[1]
# Instantiate an XGBRegressor with default hyperparameter settings
gbr_ = GradientBoostingRegressor(n_estimators = 50, random_state = 0)
# and compute a baseline to beat with hyperparameter optimization
baseline_h = cross_val_score(gbr, X, Y, scoring='neg_mean_squared_error').mean()
reticulate::repl_python()
