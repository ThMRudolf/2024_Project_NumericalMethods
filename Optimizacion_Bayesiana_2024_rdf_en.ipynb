{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finding of adequate hyper parameters is crucial to improve the performance and generalization of machine learning models. There are several methods described in modern literature that can help to find the best fitting hyper parameters. The performance is tested in different dataset that are tested and well studied, such as MINST. Hyper parameters (HP) are external to the learning process but significantly impact model performance. HP are external to the learning process but significantly impact model performance. Effective tuning typically requires domain expertise or computationally expensive methods like brute-force search.\n",
    "Effective tuning typically requires domain expertise or computationally expensive methods like brute-force search In [1, 2, 3], the Bayesian optimization is explored.  Bayesian optimization leverages prior knowledge to update posterior distributions based on sample data, guiding the search for optimal hyperparameter configurations efficiently. Traditional approaches like grid search and random search are computationally expensive and less effective in high-dimensional spaces. Bayesian optimization offers a more efficient alternative, particularly for expensive, black-box objective functions. \n",
    "In [1] the use of Bayesian optimization is explored for tuning hyper parameters in machine learning models, which is critical for improving model performance. The study proposes a method using Bayesian optimization with Gaussian processes. Traditional approaches like grid search and random search are computationally expensive and less effective in high-dimensional spaces. Bayesian optimization offers a more efficient alternative, particularly for expensive, black-box objective functions. The results were conducted on machine learning models like random forests, neural networks (CNN and RNN), and deep forests (gcForest). Bayesian optimization demonstrated significant improvements in prediction accuracy and computational efficiency compared to traditional methods.The results confirm that Bayesian optimization outperforms other optimization techniques in terms of speed and achieving better model performance, especially in scenarios with limited computational resources or high-dimensional hyperparameter spaces.\n",
    "\n",
    "In [2] it is stated that complex models like Deep Belief Networks (DBNs) have numerous hyper parameters, making manual optimization inefficient and inconsistent.\n",
    "The difficulty in tuning these models hinders reproducibility and progress in machine learning research. Therefore, the proposed approaches is based on\n",
    "Random Search, a method where hyper parameters are sampled randomly from predefined distributions. It is efficient for simple problems but struggles with complex models like DBNs. In order to improve this disadvantage, the Sequential Model-Based Optimization is introduced.\n",
    "This method approximates the loss function with a surrogate model to guide the search. Two specific variants of the methods are explained: Gaussian Process and the\n",
    "Tree-Structured Parzen Estimator. Both methods were tested on DBNs with up to 32 hyper parameters across tasks like image classification on datasets such as MNIST and MRBI. The results show, that Random search matched human manual optimization for simple tasks but failed for harder datasets.\n",
    "Tree-Structured Parzen Estimator consistently outperformed both random search and manual tuning, achieving better accuracy and efficiency.The Gaussian Process was effective but less efficient than TPE due to computational overhead.\n",
    "\n",
    "In [3], the results proves that experiments were conducted on machine learning models like random forests, neural networks (CNN and RNN), and deep forests (gcForest). Bayesian optimization demonstrated significant improvements in prediction accuracy and computational efficiency compared to traditional methods.\n",
    "The results confirm that Bayesian optimization outperforms other optimization techniques in terms of speed and achieving better model performance, especially in scenarios with limited computational resources or high-dimensional hyperparameter spaces.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Hyunghun Cho et al.: *Basic Enhancement Strategies When Using Bayesian Optimization for Hyperparameter Tuning of Deep Neural Networks*, Special section on scalable deeo learning for big data, VOLUME 8, Digital Object Identifier 10.1109/ACCESS.2020.2981072, pp. 52588-52608 IEEE Access, 2020\n",
    "\n",
    "[2] James Bergstra et al: *Algorithms for Hyper-Parameter Optimization*, NIPS'11: Proceedings of the 24th International Conference on Neural Information Processing Systems,  pp. 2546 - 2554, 2011\n",
    "\n",
    "[3] Jia Wu et al: *Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization*, Journal of Electronic Science , VOL. 17, NO. 1,Digital Object Identifier:10.11989/JEST.1674-862X.80904120, pp.26 - 40, 2019, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep data and environment and Import MINST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize\n",
    "# obejective function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "## necessary libraries to build model tensorflow\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "## \n",
    "import GPyOpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "# Split into training and testing datasets\n",
    "(X_train, X_test), (y_train, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\\\n",
    "https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html#sphx-glr-auto-examples-bayesian-optimization-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model as objective function()\n",
    "def build_model(learning_rate, units, dropout_rate, l2_reg):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape(28,28)))\n",
    "    model.add(Dense(units=units, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(Dense(units=units, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(Dense(units=1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_score(params):\n",
    "    learning_rate = float(params[:, 0])\n",
    "    units = int(params[:, 1])\n",
    "    dropout_rate = float(params[:, 2])\n",
    "    l2_reg = float(params[:, 3])\n",
    "    batch_size = int(params[:, 4])\n",
    "\n",
    "    model = build_model(learning_rate, units, dropout_rate, l2_reg)\n",
    "    \n",
    "    checkpoint_path = f'checkpoint_lr_{learning_rate}_units_{units}_dropout_{dropout_rate}_l2_{l2_reg}_batch_{batch_size}.h5'\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=50,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)\n",
    "    \n",
    "    val_acc = np.max(history.history['val_accuracy'])\n",
    "    return -val_acc\n",
    "\n",
    "# Define the bounds of the hyperparameters\n",
    "bounds = [\n",
    "    {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 1e-1)},\n",
    "    {'name': 'units', 'type': 'discrete', 'domain': (16, 32, 64, 128, 256)},\n",
    "    {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.0, 0.5)},\n",
    "    {'name': 'l2_reg', 'type': 'continuous', 'domain': (1e-6, 1e-2)},\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64, 128)}\n",
    "]\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=model_score, domain=bounds)\n",
    "optimizer.run_optimization(max_iter=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesianOptimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
