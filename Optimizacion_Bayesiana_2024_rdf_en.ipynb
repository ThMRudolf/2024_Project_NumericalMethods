{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finding of adequate hyper parameters is crucial to improve the performance and generalization of machine learning models. There are several methods described in modern literature that can help to find the best fitting hyper parameters. The performance is tested in different dataset that are tested and well studied, such as MINST. Hyper parameters (HP) are external to the learning process but significantly impact model performance. HP are external to the learning process but significantly impact model performance. Effective tuning typically requires domain expertise or computationally expensive methods like brute-force search.\n",
    "Effective tuning typically requires domain expertise or computationally expensive methods like brute-force search In [1, 2, 3], the Bayesian optimization is explored.  Bayesian optimization leverages prior knowledge to update posterior distributions based on sample data, guiding the search for optimal hyperparameter configurations efficiently. Traditional approaches like grid search and random search are computationally expensive and less effective in high-dimensional spaces. Bayesian optimization offers a more efficient alternative, particularly for expensive, black-box objective functions. \n",
    "In [1] the use of Bayesian optimization is explored for tuning hyper parameters in machine learning models, which is critical for improving model performance. The study proposes a method using Bayesian optimization with Gaussian processes. Traditional approaches like grid search and random search are computationally expensive and less effective in high-dimensional spaces. Bayesian optimization offers a more efficient alternative, particularly for expensive, black-box objective functions. The results were conducted on machine learning models like random forests, neural networks (CNN and RNN), and deep forests (gcForest). Bayesian optimization demonstrated significant improvements in prediction accuracy and computational efficiency compared to traditional methods.The results confirm that Bayesian optimization outperforms other optimization techniques in terms of speed and achieving better model performance, especially in scenarios with limited computational resources or high-dimensional hyperparameter spaces.\n",
    "\n",
    "In [2] it is stated that complex models like Deep Belief Networks (DBNs) have numerous hyper parameters, making manual optimization inefficient and inconsistent.\n",
    "The difficulty in tuning these models hinders reproducibility and progress in machine learning research. Therefore, the proposed approaches is based on\n",
    "Random Search, a method where hyper parameters are sampled randomly from predefined distributions. It is efficient for simple problems but struggles with complex models like DBNs. In order to improve this disadvantage, the Sequential Model-Based Optimization is introduced.\n",
    "This method approximates the loss function with a surrogate model to guide the search. Two specific variants of the methods are explained: Gaussian Process and the\n",
    "Tree-Structured Parzen Estimator. Both methods were tested on DBNs with up to 32 hyper parameters across tasks like image classification on datasets such as MNIST and MRBI. The results show, that Random search matched human manual optimization for simple tasks but failed for harder datasets.\n",
    "Tree-Structured Parzen Estimator consistently outperformed both random search and manual tuning, achieving better accuracy and efficiency.The Gaussian Process was effective but less efficient than TPE due to computational overhead.\n",
    "\n",
    "In [3], the results proves that experiments were conducted on machine learning models like random forests, neural networks (CNN and RNN), and deep forests (gcForest). Bayesian optimization demonstrated significant improvements in prediction accuracy and computational efficiency compared to traditional methods.\n",
    "The results confirm that Bayesian optimization outperforms other optimization techniques in terms of speed and achieving better model performance, especially in scenarios with limited computational resources or high-dimensional hyperparameter spaces.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Hyunghun Cho et al.: *Basic Enhancement Strategies When Using Bayesian Optimization for Hyperparameter Tuning of Deep Neural Networks*, Special section on scalable deeo learning for big data, VOLUME 8, Digital Object Identifier 10.1109/ACCESS.2020.2981072, pp. 52588-52608 IEEE Access, 2020\n",
    "\n",
    "[2] James Bergstra et al: *Algorithms for Hyper-Parameter Optimization*, NIPS'11: Proceedings of the 24th International Conference on Neural Information Processing Systems,  pp. 2546 - 2554, 2011\n",
    "\n",
    "[3] Jia Wu et al: *Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization*, Journal of Electronic Science , VOL. 17, NO. 1,Digital Object Identifier:10.11989/JEST.1674-862X.80904120, pp.26 - 40, 2019, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep data and environment and Import MINST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize\n",
    "# objective function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "## necessary libraries to build model tensorflow (objective funciton)\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import GPyOpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "# Split into training and testing datasets\n",
    "(X_train, y_train), (X_val, y_val) = mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "\n",
    "# flatten the input data for NN\n",
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_val = X_val.reshape(-1, 28 * 28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\\\n",
    "https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html#sphx-glr-auto-examples-bayesian-optimization-py\\\n",
    "https://www.linkedin.com/pulse/optimizing-machine-learning-models-bayesian-deep-dive-davis-joseph-qsqje?utm_source=share&utm_medium=member_android&utm_campaign=share_via\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model as objective function()\n",
    "def build_model(learning_rate, units, l2_reg):\n",
    "    model = Sequential()\n",
    "    #model.add(Flatten(input_shape=(28,28)))\n",
    "    model.add(Dense(units=units, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(Dense(units=units, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(Dense(units=1, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(params):\n",
    "    learning_rate = float(params[:, 0])\n",
    "    units = int(params[:, 1])\n",
    "    l2_reg = float(params[:, 2])\n",
    "    batch_size = int(params[:, 3])\n",
    "\n",
    "    model = build_model(learning_rate, units,  l2_reg)\n",
    "    checkpoint_folder = \"checkpoints/\"\n",
    "    checkpoint_path = checkpoint_folder +  f'checkpoint_lr_{learning_rate}_units_{units}_l2_{l2_reg}_batch_{batch_size}.keras' #h5\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=50,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)\n",
    "    \n",
    "    val_acc = np.max(history.history['val_accuracy'])\n",
    "    return -val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "y_train unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "y_test unique labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape\n",
    "print(m)\n",
    "\n",
    "print(\"y_train unique labels:\", np.unique(y_train))\n",
    "print(\"y_test unique labels:\", np.unique(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.08826900461554395_units_128_l2_0.0025758863320338404_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.07820453786567468_units_64_l2_0.0025404114473863027_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.09229421322385369_units_32_l2_0.0020721732965733506_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.048050469115064984_units_32_l2_0.008552636118009754_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.08254375011885118_units_64_l2_0.006413010397223341_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.00517534614537441_units_128_l2_0.000957983117723653_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.05717759048884798_units_64_l2_0.0035063557209888204_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.05506888655783604_units_128_l2_0.003324688410095425_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.01723814813196486_units_128_l2_0.0020906741680253266_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.09064336350922848_units_256_l2_0.0038395534071304876_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.09059106644516363_units_64_l2_0.005374008086865992_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.06855457792649122_units_128_l2_0.0025370678197871813_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.001770641270368717_units_32_l2_0.002076022204612247_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.0995073218193265_units_128_l2_0.005677371799892708_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.07996516498445268_units_32_l2_0.004607648085171249_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.07706350572438758_units_64_l2_0.007714797254412128_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.056001521647814675_units_64_l2_0.0006209014703173723_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.01706690392479091_units_256_l2_0.002630620880993102_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.05694678531854611_units_64_l2_0.0028646442368655913_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.006656730816703791_units_128_l2_0.002309876589277293_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.06939536777485117_units_64_l2_0.0016949127599735917_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.031163632059692407_units_256_l2_0.0030211520184371854_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.05672999537117739_units_256_l2_0.008123215508229725_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.06945415352660635_units_256_l2_0.007509306508283502_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.02916248927813873_units_64_l2_0.0008189438193224321_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.036510676722160365_units_256_l2_0.009480060754903562_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.04303675885944374_units_32_l2_0.0002774366472680507_batch_128.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.020437276191862582_units_256_l2_0.008149483816997103_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.032030013452061364_units_128_l2_0.003246998649590722_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.013309008933773533_units_64_l2_0.009886084683750835_batch_64.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.051543467725800034_units_128_l2_0.007837919297145524_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.08569063933409833_units_64_l2_0.009516969090913974_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.018565507758851976_units_256_l2_0.001149528722354779_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.07142318236130658_units_32_l2_0.0025617633490320206_batch_16.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11350, saving model to checkpoints/checkpoint_lr_0.06813852607156881_units_256_l2_0.0060234788396270325_batch_32.keras\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.11350\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.11350\n"
     ]
    }
   ],
   "source": [
    "# Define the bounds of the hyperparameters\n",
    "bounds = [\n",
    "    {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-5, 1e-1)},\n",
    "    {'name': 'units', 'type': 'discrete', 'domain': (16, 32, 64, 128, 256)},\n",
    "    {'name': 'l2_reg', 'type': 'continuous', 'domain': (1e-6, 1e-2)},\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64, 128)}\n",
    "]\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=model_score, \n",
    "                                                domain=bounds, \n",
    "                                                acquisition_type='EI'  # Expected Improvement\n",
    ")\n",
    "res = optimizer.run_optimization(max_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"checkpoints\"\n",
    "\n",
    "# Get all file names in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# Print the file names\n",
    "file_names_vec = []\n",
    "for file_name in file_names:\n",
    "    file_names_vec.append(file_name)\n",
    "\n",
    "def get_BO_values(filenames):\n",
    "    \n",
    "    for(filename in file_names):\n",
    "        # Input string\n",
    "        input_string = filename\n",
    "\n",
    "        # Regular expression to extract key-value pairs\n",
    "        pattern = r\"(\\w+)_([\\d\\.]+)\"  # Matches \"key_value\" pairs\n",
    "        matches = re.findall(pattern, input_string)\n",
    "\n",
    "        # Convert to dictionary\n",
    "        parameters = {key: float(value) if '.' in value else int(value) for key, value in matches}\n",
    "\n",
    "        # Access specific parameters\n",
    "        lr = round(parameters.get(\"lr\", 0), 3)\n",
    "        units = parameters.get(\"units\", 0)\n",
    "        l2 = round(parameters.get(\"l2\", 0), 3)\n",
    "        batch = parameters.get(\"batch\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_lr_0.00517534614537441_units_128_l2_0.000957983117723653_batch_64.keras'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_vec[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesianOptimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
