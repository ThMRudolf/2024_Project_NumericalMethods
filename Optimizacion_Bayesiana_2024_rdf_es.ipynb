{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hallazgo de hiperparámetros adecuados es crucial para mejorar el rendimiento y la generalización de los modelos de aprendizaje automático. Existen varios métodos descritos en la literatura moderna que pueden ayudar a encontrar los hiperparámetros que mejor se ajustan. El rendimiento se prueba en diferentes conjuntos de datos que se prueban y se estudian bien, como MINST. Los hiperparámetros (HP) son externos al proceso de aprendizaje, pero afectan significativamente el rendimiento del modelo. Los HP son externos al proceso de aprendizaje, pero afectan significativamente el rendimiento del modelo. Un ajuste efectivo generalmente requiere experiencia en el dominio o métodos computacionalmente costosos como la búsqueda de fuerza bruta. Un ajuste efectivo generalmente requiere experiencia en el dominio o métodos computacionalmente costosos como la búsqueda de fuerza bruta En [1, 2, 3], se explora la optimización bayesiana. La optimización bayesiana aprovecha el conocimiento previo para actualizar las distribuciones posteriores en función de los datos de muestra, lo que guía la búsqueda de configuraciones óptimas de hiperparámetros de manera eficiente. Los enfoques tradicionales como la búsqueda en cuadrícula y la búsqueda aleatoria son computacionalmente costosos y menos efectivos en espacios de alta dimensión. La optimización bayesiana ofrece una alternativa más eficiente, particularmente para funciones objetivo de caja negra costosas. \n",
    "\n",
    "En [1] se explora el uso de la optimización bayesiana para ajustar los hiperparámetros en los modelos de aprendizaje automático, lo que es fundamental para mejorar el rendimiento del modelo. El estudio propone un método que utiliza la optimización bayesiana con procesos gaussianos. Los enfoques tradicionales, como la búsqueda en cuadrícula y la búsqueda aleatoria, son computacionalmente costosos y menos efectivos en espacios de alta dimensión. La optimización bayesiana ofrece una alternativa más eficiente, particularmente para funciones objetivo de caja negra costosas. Los resultados se realizaron en modelos de aprendizaje automático como bosques aleatorios, redes neuronales (CNN y RNN) y bosques profundos (gcForest). La optimización bayesiana demostró mejoras significativas en la precisión de la predicción y la eficiencia computacional en comparación con los métodos tradicionales. Los resultados confirman que la optimización bayesiana supera a otras técnicas de optimización en términos de velocidad y logro de un mejor rendimiento del modelo, especialmente en escenarios con recursos computacionales limitados o espacios de hiperparámetros de alta dimensión.\n",
    "\n",
    "En [2] se afirma que los modelos complejos como las redes de creencias profundas (DBN) tienen numerosos hiperparámetros, lo que hace que la optimización manual sea ineficiente e inconsistente. La dificultad para ajustar estos modelos obstaculiza la reproducibilidad y el progreso en la investigación del aprendizaje automático. Por lo tanto, los enfoques propuestos se basan en la Búsqueda aleatoria, un método en el que los hiperparámetros se muestrean aleatoriamente a partir de distribuciones predefinidas. Es eficiente para problemas simples, pero tiene dificultades con modelos complejos como las DBN. Para mejorar esta desventaja, se presenta la Optimización basada en modelos secuenciales. Este método aproxima la función de pérdida con un modelo sustituto para guiar la búsqueda. Se explican dos variantes específicas de los métodos: el proceso gaussiano y el estimador Parzen estructurado en árbol. Ambos métodos se probaron en DBN con hasta 32 hiperparámetros en tareas como la clasificación de imágenes en conjuntos de datos como MNIST y MRBI. Los resultados muestran que la búsqueda aleatoria coincidió con la optimización manual humana para tareas simples, pero falló para conjuntos de datos más difíciles. El estimador Parzen estructurado en árbol superó consistentemente tanto la búsqueda aleatoria como el ajuste manual, logrando una mejor precisión y eficiencia. El proceso gaussiano fue efectivo pero menos eficiente que el TPE debido a la sobrecarga computacional.\n",
    "\n",
    "En [3], los resultados demuestran que se realizaron experimentos en modelos de aprendizaje automático como bosques aleatorios, redes neuronales (CNN y RNN) y bosques profundos (gcForest). La optimización bayesiana demostró mejoras significativas en la precisión de la predicción y la eficiencia computacional en comparación con los métodos tradicionales. Los resultados confirman que la optimización bayesiana supera a otras técnicas de optimización en términos de velocidad y logro de un mejor rendimiento del modelo, especialmente en escenarios con recursos computacionales limitados o espacios de hiperparámetros de alta dimensión.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Hyunghun Cho et al.: *Basic Enhancement Strategies When Using Bayesian Optimization for Hyperparameter Tuning of Deep Neural Networks*, Special section on scalable deeo learning for big data, VOLUME 8, Digital Object Identifier 10.1109/ACCESS.2020.2981072, pp. 52588-52608 IEEE Access, 2020\n",
    "\n",
    "[2] James Bergstra et al: *Algorithms for Hyper-Parameter Optimization*, NIPS'11: Proceedings of the 24th International Conference on Neural Information Processing Systems,  pp. 2546 - 2554, 2011\n",
    "\n",
    "[3] Jia Wu et al: *Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization*, Journal of Electronic Science , VOL. 17, NO. 1,Digital Object Identifier:10.11989/JEST.1674-862X.80904120, pp.26 - 40, 2019, "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
